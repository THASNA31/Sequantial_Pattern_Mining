{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thasnai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Warehouse Associate receives orders from above personnel.', 'Warehouse Associate puts the orders together using the signoff sheet or the schedule of what is being delivered or picked up.', 'Once all orders are pulled, orders are double-checked for the Agencies.', 'If there were any orders that had problems the Warehouse Associate will investigate any missing product.', 'If warehouse is out of the product, the warehouse supervisor or inventory control supervisor will determine what product will be distributed instead.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "p = \"Warehouse Associate receives orders from above personnel.  Warehouse Associate puts the orders together using the signoff sheet or the schedule of what is being delivered or picked up. Once all orders are pulled, orders are double-checked for the Agencies.  If there were any orders that had problems the Warehouse Associate will investigate any missing product. If warehouse is out of the product, the warehouse supervisor or inventory control supervisor will determine what product will be distributed instead.\"\n",
    "docs  = tokenize.sent_tokenize(p)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['warehouse associate receives orders personnel .',\n",
       " 'warehouse associate puts orders together using signoff sheet schedule delivered picked .',\n",
       " 'orders pulled , orders double-checked agencies .',\n",
       " 'orders problems warehouse associate investigate missing product .',\n",
       " 'warehouse product , warehouse supervisor inventory control supervisor determine product distributed instead .']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [d.lower() for d in docs]\n",
    "filtered_docs = []\n",
    "for doc in docs:\n",
    "    word_tokens = word_tokenize(doc) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    ftdoc = \" \".join(filtered_sentence)\n",
    "    filtered_docs.append(ftdoc)\n",
    "\n",
    "filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer().fit(filtered_docs)\n",
    "tokenizer = count_vect.build_tokenizer()\n",
    "# count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = []\n",
    "for doc in filtered_docs:\n",
    "    x = []\n",
    "    for token in tokenizer(doc):\n",
    "        x.append(count_vect.vocabulary_.get(token))\n",
    "    input_array.append(x)\n",
    "    \n",
    "max_len = max([len(d) for d in input_array])\n",
    "input_array = pad_sequences(input_array, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26,  1, 19, 12, 13,  0,  0,  0,  0,  0,  0],\n",
       "       [26,  1, 18, 12, 24, 25, 22, 21, 20,  4, 14],\n",
       "       [12, 17, 12,  7,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [12, 15, 26,  1, 10, 11, 16,  0,  0,  0,  0],\n",
       "       [26, 16, 26, 23,  9,  3, 23,  5, 16,  6,  8]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefixspan import PrefixSpan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, [26]), (3, [26, 1]), (2, [26, 1, 12]), (2, [26, 1, 0]), (2, [26, 1, 0, 0]), (2, [26, 1, 0, 0, 0]), (2, [26, 1, 0, 0, 0, 0]), (2, [26, 12]), (2, [26, 0]), (2, [26, 0, 0]), (2, [26, 0, 0, 0]), (2, [26, 0, 0, 0, 0]), (2, [26, 16]), (3, [1]), (2, [1, 12]), (2, [1, 0]), (2, [1, 0, 0]), (2, [1, 0, 0, 0]), (2, [1, 0, 0, 0, 0]), (4, [12]), (3, [12, 0]), (3, [12, 0, 0]), (3, [12, 0, 0, 0]), (3, [12, 0, 0, 0, 0]), (2, [12, 0, 0, 0, 0, 0]), (2, [12, 0, 0, 0, 0, 0, 0]), (3, [0]), (3, [0, 0]), (3, [0, 0, 0]), (3, [0, 0, 0, 0]), (2, [0, 0, 0, 0, 0]), (2, [0, 0, 0, 0, 0, 0]), (2, [16])]\n"
     ]
    }
   ],
   "source": [
    "ps = PrefixSpan(input_array)\n",
    "print(ps.frequent(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
